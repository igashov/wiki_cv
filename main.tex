\documentclass[]{article}
\usepackage{cite}
\usepackage{indentfirst}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

% ----  ADDED IN OVERLEAF ---

\usepackage{geometry}
\geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
}

\usepackage{titlesec}
\titlelabel{\thetitle.\quad}

% ----------------------------

\let\v\mathbf

%opening
\title{Обзор сверточных нейронных сетей на графах}
\author{Илья Игашов}

\begin{document}

\maketitle

В последние годы значительно возрос интерес научного сообщества к вопросу создания алгоритмов машинного обучения на нерегулярных структурах данных, таких как графы и выпуклые многообразия. Учитывая то, насколько успешным оказалось использование сверточных нейронных сетей в области компьютерного зрения, в последние годы особо остро встал вопрос применения операции свертки к нерегулярным структурам типа графов. В силу отсутствия четкой структуры на множестве узлов графа, вопрос построения свертки в данном случае становится нетривиальным, так как в классической теории обработки сигналов свертка определеятся через оператор трансляции, смысл которого утрачивается, если мы говорим о нерегулярных структурах типа графов.

В данном обзоре мы рассмотрим два метода определения операции свертки на графах: спектральный и пространственный. Спектральный метод основан на применении теории Фурье к графам\cite{shuman2013emerging} -- в рамках этого подхода удалось получить математическое выражение для операции свертки на графах, а также использовать полученную операцию для обучения сверточных нейронных сетей. Второй метод, пространственный, основан на более житейском и логическом (и менее математическом) подходе к формулированию понятия свертки в терминах графов, он является более интерпретируемым и универсальным.

Как мы увидим ниже, в случае с графами сверточным нейросетям необходимо, чтобы узел в графе представлялся как вектор признаков. Одним из возможных методов синтеза признаков является получение эмбеддингов. В данном обзоре мы познакомимся с простым способом отображать узлы графа в $d$-мерное вещественное пространство \cite{hamilton2017representation}.

\section{Обозначения и постановка задачи}
Будем рассматривать неориентированный взвешенный связный граф $\mathcal{G}=\{\mathcal{V}, \mathcal{E}\}$, где $\mathcal{V}\ -$ множество вершин, $|\mathcal{V}|=N$, $\mathcal{E}\ -$ множество ребер, $\mathcal{E}\subset\mathcal{V}\times\mathcal{V}$. Для удобства пронумеруем все вержины в графе от $1$ до $N$ и будем считать, что $\mathcal{V}=\{1,\dots,N\}$. Обозначим через $\v{W}$ матрицу смежности графа: $\v{W}_{ij}=w(i, j)I_{\mathcal{E}}(i,j)$, где $w:\mathcal{E}\to\mathbb{R}$ -- функция веса ребер графа $\mathcal{G}$, а $I_{\mathcal{E}}$ -- индикаторная функция множества $\mathcal{E}$. Степенью $i$-й вершины графа $\mathcal{G}$ называется величина $d_i=\sum_{j\in\mathcal{N}_i}\v{W}_{ij}$, где $\mathcal{N}_i=\{j\in\mathcal{V}: (i,j)\in\mathcal{E}\}\ -$ множество соседей вершины $i$.

Будем считать, что каждая вершина графа обладает вектором параметров $\v{x}\in\mathbb{R}^m$, и, таким образом, граф может быть предствален парой двух матриц: $(\v{X}, \v{W})$, где $\v{X}=(\v{x}_1,\dots,\v{x}_N)^{\top}$.

Следует отметить, что существует множество различных задач, связанных с графами. В частности, в задачах машинного обучения на графах целевая функция может быть определена как на множесте графов, так и на множестве вершин графов. Для простоты в этом обзоре мы будем говорить о втором варианте.

\section{Спектральный подход}

\subsection*{Спектральная теория графов}
Рассмотрим некоторую функцию $f:\mathcal{V}\to\mathbb{R}$, определенную на множестве $\mathcal{V}$ вершин графа $\mathcal{G}$. Функцию $f$ можно представить как вектор $\v{f}\in\mathbb{R}^N$ (сигнал функции $f$), $i$-я компоненкта которого равна значению функции $f$ на $i$-й вершине графа $\mathcal{G}$ (в наших терминах сигнал является признаком вершины графа $\mathcal{G}$). Определим Лапласиан графа $\mathcal{G}$ как матрицу $\v{L}=\v{D}-\v{W}$, где $\v{D}=\text{diag}\{d_1,\dots,d_N\}\ -$ диагональная матрица степеней верщин графа $\mathcal{G}$. Лапласиан является оператором разности на множестве функций $f:\mathcal{V}\to\mathbb{R}$, поскольку, как легко заметить, для $\v{f}\in\mathbb{R}^N$ справедливо равенство:
\begin{align}\label{laplacian}
    (\v{L}f)(i)=\sum_{j\in\mathcal{N}_i}\v{W}_{ij}[f(i)-f(j)].
\end{align}
Поскольку Лапласиан графа является симметричной вещественной матрицей, для нее существует полный набор ортонормированных собственных векторов $\{\v{u}_l\}_{l=1}^{N}$ и соответствующий ему набор неотрицательных вещественных собственных значений (или частот, по аналогии с частотами сигнала в разложении Фурье) $\{\lambda_l\}_{l=1}^{N}$. Положим $\Lambda=\text{diag}(\lambda_1,\dots,\lambda_N)$.

% Поэтому будем считать, что $0=\lambda_1\leq\lambda_2\leq\dots\leq\lambda_N$, и определим спектр Лапласиана $\v{L}$ как $\sigma(\v{L})=\{\lambda_1,\dots,\lambda_N\}$.

Обратимся к классической теории обработки сигналов и к теории Фурье в пространстве вещественных функций. Если поставить в соответствие бесконечномерному базису $\{e^{2\pi{i}\xi{t}}\}_{\xi\in\mathbb{R}}$ одномерного Лапласиана $\Delta_t$ наш набор собственных векторов $\{\v{u}_l\}_{l=1}^{N}$, а множеству частот ${\xi\in\mathbb{R}}\ -$ наш набор собственных значений $\{\lambda_l\}_{l=1}^{N}$, то мы сможем определить преобразование Фурье на пространстве функций $f:\mathcal{V}\to\mathbb{R}$:
\begin{align}\label{fourier}
    \hat{f}(\lambda_l)=\langle\v{f},\v{u}_l\rangle=\sum_{i=1}^{N}f(i)u_l(i).
\end{align}
Также можно получить формулу для обратного преобразования Фурье:
\begin{align}
    f(i)=\sum_{l=0}^{N}\hat{f}(\lambda_l)u_l(i).
\end{align}

\subsection*{Операция свертки}

Особенностью работы с графами является то, что множество вершин графа не обладает четкой и однозначной структурой. В частности, для функций на графах невозможно определить оператор трансляции, потому что попросту непонятно, что значит "$i-j$"\ для двух вершин $i,j\in\mathcal{V}$. Этот факт не позволяет использовать оригинальное определение операции свертки:
\begin{align}
    (f*g)(t)=\int_{\mathbb{R}}{f(\tau)g(t-\tau)d\tau}.
\end{align}
Но тут к нам на помощь приходит понятие фильтра из классической теории обработки сигналов. Фильтром называется функция $\hat{h}$, роль которой $-$ усиливать или ослаблять вклад каких-либо частот $\xi$ в выходной сигнал:
\begin{align}\label{convolut}
    \hat{f}_{\text{out}}(\xi)=\hat{f}_{\text{in}}(\xi)\hat{h}(\xi).
\end{align}
С помощью обратного преобразования Фурье выходного сигнала можно получить операцию свертки:
\begin{align}
    f_{\text{out}}(t)=\int_{\mathbb{R}}{\hat{f}_{\text{in}}(\xi)\hat{h}(\xi)e^{2\pi{i}\xi{t}}d\xi}=\int_{\mathbb{R}}{f_{\text{in}}(\tau)h(t-\tau)d\tau}=(f_{\text{in}}*h)(t).
\end{align}
Таким образом, можно записать определение операции свертки для функций на графе $\mathcal{G}$:
\begin{align}\label{spectral_conv}
    (f*g)(i)=\sum_{l=1}^{N}\hat{f}(\lambda_l)\hat{g}(\lambda_l)u_l(i).
\end{align}
Пользуясь формулой \eqref{fourier}, запишем формулу свертки в матричном виде:
\begin{align}\label{mat_conv}
    (f*g)(i)=\sum_{l=1}^{N}\langle\v{f},\v{u}_l\rangle\langle\v{g},\v{u}_l\rangle u_l(i)=\v{U}(\v{U}^{\top}\v{f}\odot\v{U}^{\top}\v{g})=\v{U}\v{G}\v{U}^{\top}\v{f},
\end{align}
где $\v{U}=(\v{u}_1,\dots,\v{u}_N)$ -- матрица базисных векторов, $\odot$ -- поэлементное произведение, а $\v{G}=\text{diag}(\v{U}^{\top}\v{g})$.

\subsection*{Спектральные сверточные сети на графах}

Результат свертки полностью зависит от значений матрицы $\v{G}$, и она может выступать, например, непараметрическим фильтром, т.е. матрицей, где все параметры оптимизируются. Так мы получаем формулу для сверточного слоя из работы Spectral Convolutional Neural Network\cite{bruna2013spectral}:
\begin{align}
    \v{f}'_{j}=\sigma\left(\sum_{i=1}^{d_{\text{in}}}\v{U}\v{G}_{ij}(\theta)\v{U}^{\top}\v{f}_{i}\right),
\end{align}
где на вход сверточному слою подается $\v{F}=(\v{f}_1,\dots,\v{f}_{d_{\text{in}}})$ -- матрица входного сигнала, $\v{G}(\theta)\in\mathbb{R}^{d_{\text{in}}\times{d_{\text{out}}}}$ -- обучаемая матрица параметров, $\v{F}'=(\v{f}_1,\dots,\v{f}_{d_{\text{out}}})$ -- матрица выходного сигнала.

Одним из недостатков такого подхода является то, что непараметрический фильтр не обладает свойством локализации: поскольку элементы диагонали матрицы $\v{G}$ из формулы \eqref{mat_conv} соответствуют коэффициентам разложения сигнала в ряд Фурье, а значит, и определенным частотам $\lambda_i$, то можно ввести явную зависимость от различных собственных значений в конструкцию оптимизируемых параметров. Например, можно оптимизировать коэффициенты в полиноме $r$-ой степени от матрицы частот $\Lambda$:
\begin{align}
    \v{G}(\theta)=\sum_{j=0}^{r-1}\theta_j\Lambda^j,
\end{align}
или можно построить параметр с помощью полиномов Чебышева\cite{defferrard2016convolutional}
\begin{align}
    T_j(x)&=2xT_{j-1}(x)-T_{j-2}(x),\\
    T_0(x)&=1,\\
    T_1(x)&=x,
\end{align}
и отнормированной матрицы частот $\hat\Lambda=2\lambda_N^{-1}\Lambda-\v{I}$:
\begin{align}
    \v{G}(\theta)&=\sum_{j=0}^{r-1}\theta_j\v{U}T_j(\hat\Lambda)\v{U}^{\top}.
\end{align}

Спектральная свертка обладает красивой математической базой, однако в данном методе существует один большой практический недостаток. Поскольку Лапласиан графа напрямую связан с топологией графа, спектральные сверточные сети не могут быть применены к различным графам, так как у каждого графа будет свой Лапласиан. Это обстоятельство резко сокращает круг задач, в которых спектральная свертка может найти себе применение.

\section{Пространственный подход}

В основе пространственного подхода лежит идея о том, что информация о вершине графа содержится не только в признаках самой вершины, но и в признаках соседей этой вершины. По аналогии с тем, как в классических 2D-свертках небольшой фильтр "сканирует"\ пиксели изображения, в случае с графами было предложено "сканировать"\ каждый узел вместе с его соседями. 

Примером такого механизма является сверточный слой из работы Neural Networks For Graphs\cite{micheli2009neural}:
\begin{align}
    \v{H}'=f\left(\v{X}\v{\Theta}+\v{W}\v{H}\v{\Xi}\right),
\end{align}
где $\v{H}=(\v{h}_1,\dots,\v{h}_N)$ --  матрица входных признаков вершин, $\v{h}_i\in\mathbb{R}^{d_{\text{in}}}$, $\v{H}'=(\v{h}'_1,\dots,\v{h}'_N)$ --  матрица выходных признаков вершин, $\v{h}'_i\in\mathbb{R}^{d_{\text{out}}}$, $\v{X}$ -- матрица изначальных признаков вершин, $\v{\Theta}$ и $\v{\Xi}$ -- матрицы оптимизируемых параметров, $f$ -- функция активации.

С развитием механизма пространственной свертки появилось понятие "Message-Passing Network"\cite{gilmer2017neural}. Поскольку для каждой вершины один слой захватывает ее соседей, благодаря нескольким сверточным слоям информация перемещается между несмежными вершинами графа. В общем виде формула для Message-Passing-слоя выглядит следующим образом:
\begin{align}
    \v{h}'_i=U\left(\v{h}_i, \sum_{j\in\mathcal{N}_i}M\left(\v{h}_i, \v{h}_j, \v{W}_{ij}\right)\right),
\end{align}
где $U$ и $M$ -- функции с оптимизируемыми параметрами. 

Существует множество вариантов пространственных сверточных слоев, некоторые из них уже реализованы в фреймворке PyTorch Geometric\cite{fey2019fast}. В целом пространственный подход пользуется большой популярностью засчет своей простоты и универсальности. 

\section{Эмбеддинги}

Наконец, рассмотрим механизм построения эмбеддингов узлов графа $\mathcal{G}$. Определим функции энкодера и декодера. 
\begin{align}
    \text{ENC}&:\mathcal{V}\to\mathbb{R}^d,\\
    \text{DEC}&:\mathbb{R}^d\times\mathbb{R}^d\to\mathbb{R}_+.
\end{align}
Энкодер будет переводить узел графа в $d$-мерный вещественный вектор. Попарный декодер, получая на вход эмбеддинги двух узлов, восстанавливает число -- меру близости этих узлов в графе $\mathcal{G}$:
\begin{align}
    \text{DEC}\left(\text{ENC}(i),\text{ENC}(j)\right)=\text{DEC}\left(\v{z}_i,\v{z}_j\right)\approx s_{\mathcal{G}}(i,j),
\end{align}
где $s_{\mathcal{G}}(i,j)$ может быть, например, величиной кратчайшего пути между вершинами $i$ и $j$ в графе $\mathcal{G}$, или вероятностью того, что в процессе случайного блуждания фиксированной длины со стартом в вершине $i$ вершина $j$ будет посещена.
Функция потерь в процессе обучения выглядит следующим образом:
\begin{align}
    \mathcal{L}=\sum_{(i,j)\in\mathcal{E}}l\left(\text{DEC}\left(\v{z}_i,\v{z}_j\right), s_{\mathcal{G}}(i,j)\right).
\end{align}
Энкодер может быть любой моделью, на вход ему обычно подается матрица с one-hot-представлением узлов. От выбора декодера часто зависит выбор функции потери. Например, в качестве декодера можно взять расстояние между векторами, а в качестве функции потери -- произведение:
\begin{align}
    &\text{DEC}(\v{z}_i, \v{z}_j)=\|\v{z}_i-\v{z}_j\|_2^2\\
    &\mathcal{L}=\sum_{(i,j)\in\mathcal{E}}\text{DEC}(\v{z}_i, \v{z}_j)s_{\mathcal{G}}(i, j).
\end{align}
Другой вариант -- это скалярное произведение и сумма квадратов разности:
\begin{align}
    &\text{DEC}(\v{z}_i, \v{z}_j)=\v{z}_i^{\top}\v{z}_j\\
    &\mathcal{L}=\sum_{(i,j)\in\mathcal{E}}\left(\text{DEC}(\v{z}_i, \v{z}_j)-s_{\mathcal{G}}(i, j)\right)^2.
\end{align}
Как и в случае со спектральными свертками, главным недостатком данного класса методов является то, что для одной модели топология графов, которые она использует, должна быть одной и той же.

\bigskip

\bibliography{literature}
\bibliographystyle{plain}

\end{document}

